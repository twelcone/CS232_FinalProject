{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12513,"status":"ok","timestamp":1653204255386,"user":{"displayName":"An Võ Khánh","userId":"03935501821652710257"},"user_tz":-420},"id":"HcVSkMehl_fJ","outputId":"5a557d49-f348-4be9-9290-8a68174b5395"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[K     |████████████████████████████████| 85.5 MB 96 kB/s \n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n"]}],"source":["!pip install faiss-gpu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnHKF2bxTjpG"},"outputs":[],"source":["import glob\n","from itertools import chain\n","import os\n","import random\n","import zipfile\n","from tqdm.notebook import tqdm\n","import pickle\n","import time\n","import copy\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","import cv2\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import datasets, transforms, models\n","import faiss                            \n","from torch.optim.lr_scheduler import ReduceLROnPlateau"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Orj4y_GAhsbC"},"outputs":[],"source":["PATH_TRAIN = \"/content/drive/MyDrive/COURSES/CS232/CBMIR/ct_dataset_split/train\"\n","PATH_VALID = \"/content/drive/MyDrive/COURSES/CS232/CBMIR/ct_dataset_split/val\""]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"k08L5faGqAlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pil_loader(path):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        with Image.open(f) as img:\n","            return img.convert('RGB')"],"metadata":{"id":"l3lvzp9D1fQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transforms = {\n","    'train': transforms.Compose([\n","                        transforms.RandomResizedCrop(224),\n","                        transforms.ToTensor(),\n","                        transforms.Normalize(\n","                            [0.485, 0.456, 0.406], \n","                            [0.229, 0.224, 0.225]\n","                        )\n","    ]),\n","    'val': transforms.Compose([\n","                        transforms.Resize(224),\n","                        transforms.CenterCrop(224),\n","                        transforms.ToTensor(),\n","                        transforms.Normalize(\n","                            [0.485, 0.456, 0.406], \n","                            [0.229, 0.224, 0.225]\n","                        )\n","    ]),\n","}\n","\n","data_dir = '/content/drive/MyDrive/COURSES/CS232/CBMIR/ct_dataset_split/'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),  data_transforms[x])\n","                  for x in ['train', 'val']}\n","\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n","              for x in ['train', 'val']}\n","\n","dataset_sizes = {x: len(image_datasets[x]) \n","                for x in ['train', 'val']}\n","\n","class_names = image_datasets['train'].classes \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRgBb84X9ikR","executionInfo":{"status":"ok","timestamp":1653193638105,"user_tz":-420,"elapsed":6,"user":{"displayName":"An Võ Khánh","userId":"03935501821652710257"}},"outputId":"660e80a4-8fe1-4dfb-f2e0-caacba53e793"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"QaYTRCpjwiNH"}},{"cell_type":"code","source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25, save_log=False):\n","    history = {\n","        'all_epoch_info': []\n","    }\n","    train = {\n","\n","    }\n","    val = {\n","\n","    }\n","    \n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch + 1}/{num_epochs}')\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            # if phase == 'train':\n","            #     scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","            if phase == 'train':\n","                train = {\n","                    'loss': epoch_loss,\n","                    'acc': epoch_acc.item()\n","                }\n","            if phase == 'val':\n","                val = {\n","                    'loss': epoch_loss,\n","                    'acc': epoch_acc.item()\n","                }\n","\n","            # deep copy the model\n","            if phase=='val':\n","                scheduler.step(epoch_loss)\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                torch.save(model.state_dict(), '/content/drive/MyDrive/COURSES/CS232/CBMIR//model/CBMIR_resnet18_method3.pt')\n","\n","        history['all_epoch_info'].append({\n","            'epoch': epoch,\n","            'info': { \n","                'train': train,\n","                'val': val\n","            }\n","        })\n","        # print(history)\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best val Acc: {best_acc:4f}')\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model if not save_log else model, history"],"metadata":{"id":"tki8LeT_9UwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"id":"rXyBRG-oqeTv","executionInfo":{"status":"error","timestamp":1653204461598,"user_tz":-420,"elapsed":480,"user":{"displayName":"An Võ Khánh","userId":"03935501821652710257"}},"outputId":"eef1d0f8-a107-48f2-b203-a0305dd23d31"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6173a84c43c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"]}]},{"cell_type":"code","source":["model_ft = models.resnet18(pretrained=True)\n","cnt = 0\n","for param in model_ft.parameters():\n","    if cnt < 61 * 3 // 4:\n","        param.requires_grad = False\n","    else:\n","        break\n","    cnt += 1\n","num_ftrs = model_ft.fc.in_features\n","# Here the size of each output sample is set to 3.\n","# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n","model_ft.fc = nn.Linear(num_ftrs, 7)\n","\n","model_ft = model_ft.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n","\n","scheduler = ReduceLROnPlateau(optimizer_ft, 'min', factor=0.1, patience=3, min_lr=1e-6, verbose=True)"],"metadata":{"id":"6-PKrRJcsnYj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","summary(model_ft, (3, 224, 224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kpy0ydWlqk09","executionInfo":{"status":"ok","timestamp":1653204486236,"user_tz":-420,"elapsed":503,"user":{"displayName":"An Võ Khánh","userId":"03935501821652710257"}},"outputId":"95318c6c-f0de-4599-b8dc-8b0613547684"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-20          [-1, 128, 28, 28]             256\n","             ReLU-21          [-1, 128, 28, 28]               0\n","           Conv2d-22          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-23          [-1, 128, 28, 28]             256\n","           Conv2d-24          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-25          [-1, 128, 28, 28]             256\n","             ReLU-26          [-1, 128, 28, 28]               0\n","       BasicBlock-27          [-1, 128, 28, 28]               0\n","           Conv2d-28          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-29          [-1, 128, 28, 28]             256\n","             ReLU-30          [-1, 128, 28, 28]               0\n","           Conv2d-31          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-36          [-1, 256, 14, 14]             512\n","             ReLU-37          [-1, 256, 14, 14]               0\n","           Conv2d-38          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-39          [-1, 256, 14, 14]             512\n","           Conv2d-40          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-41          [-1, 256, 14, 14]             512\n","             ReLU-42          [-1, 256, 14, 14]               0\n","       BasicBlock-43          [-1, 256, 14, 14]               0\n","           Conv2d-44          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-45          [-1, 256, 14, 14]             512\n","             ReLU-46          [-1, 256, 14, 14]               0\n","           Conv2d-47          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-48          [-1, 256, 14, 14]             512\n","             ReLU-49          [-1, 256, 14, 14]               0\n","       BasicBlock-50          [-1, 256, 14, 14]               0\n","           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n","             ReLU-53            [-1, 512, 7, 7]               0\n","           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n","           Conv2d-56            [-1, 512, 7, 7]         131,072\n","      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n","             ReLU-58            [-1, 512, 7, 7]               0\n","       BasicBlock-59            [-1, 512, 7, 7]               0\n","           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n","             ReLU-62            [-1, 512, 7, 7]               0\n","           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n","             ReLU-65            [-1, 512, 7, 7]               0\n","       BasicBlock-66            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","           Linear-68                    [-1, 7]           3,591\n","================================================================\n","Total params: 11,180,103\n","Trainable params: 8,397,319\n","Non-trainable params: 2,782,784\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 62.79\n","Params size (MB): 42.65\n","Estimated Total Size (MB): 106.01\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["model_ft, history = train_model(model_ft, criterion, optimizer_ft, scheduler=scheduler, num_epochs=50, save_log=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDzd9305AZrT","executionInfo":{"status":"ok","timestamp":1653194357261,"user_tz":-420,"elapsed":691401,"user":{"displayName":"An Võ Khánh","userId":"03935501821652710257"}},"outputId":"8a27a6b1-d18c-438d-91ff-9663227c3dd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1604 Acc: 0.9484\n","val Loss: 0.0930 Acc: 0.9761\n","\n","Epoch 2/50\n","----------\n","train Loss: 0.0712 Acc: 0.9801\n","val Loss: 0.0138 Acc: 0.9935\n","\n","Epoch 3/50\n","----------\n","train Loss: 0.0452 Acc: 0.9876\n","val Loss: 0.0534 Acc: 0.9826\n","\n","Epoch 4/50\n","----------\n","train Loss: 0.0291 Acc: 0.9901\n","val Loss: 0.0198 Acc: 0.9957\n","\n","Epoch 5/50\n","----------\n","train Loss: 0.0678 Acc: 0.9826\n","val Loss: 0.0152 Acc: 0.9957\n","\n","Epoch 6/50\n","----------\n","train Loss: 0.0485 Acc: 0.9870\n","val Loss: 0.0450 Acc: 0.9870\n","Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n","\n","Epoch 7/50\n","----------\n","train Loss: 0.0370 Acc: 0.9870\n","val Loss: 0.0252 Acc: 0.9913\n","\n","Epoch 8/50\n","----------\n","train Loss: 0.0165 Acc: 0.9938\n","val Loss: 0.0182 Acc: 0.9935\n","\n","Epoch 9/50\n","----------\n","train Loss: 0.0169 Acc: 0.9919\n","val Loss: 0.0212 Acc: 0.9913\n","\n","Epoch 10/50\n","----------\n","train Loss: 0.0208 Acc: 0.9919\n","val Loss: 0.0159 Acc: 0.9978\n","Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n","\n","Epoch 11/50\n","----------\n","train Loss: 0.0145 Acc: 0.9963\n","val Loss: 0.0154 Acc: 0.9957\n","\n","Epoch 12/50\n","----------\n","train Loss: 0.0132 Acc: 0.9957\n","val Loss: 0.0236 Acc: 0.9935\n","\n","Epoch 13/50\n","----------\n","train Loss: 0.0140 Acc: 0.9950\n","val Loss: 0.0173 Acc: 0.9957\n","\n","Epoch 14/50\n","----------\n","train Loss: 0.0111 Acc: 0.9963\n","val Loss: 0.0159 Acc: 0.9957\n","Epoch 00014: reducing learning rate of group 0 to 1.0000e-06.\n","\n","Epoch 15/50\n","----------\n","train Loss: 0.0147 Acc: 0.9969\n","val Loss: 0.0155 Acc: 0.9957\n","\n","Epoch 16/50\n","----------\n","train Loss: 0.0142 Acc: 0.9950\n","val Loss: 0.0167 Acc: 0.9957\n","\n","Epoch 17/50\n","----------\n","train Loss: 0.0097 Acc: 0.9969\n","val Loss: 0.0189 Acc: 0.9957\n","\n","Epoch 18/50\n","----------\n","train Loss: 0.0096 Acc: 0.9981\n","val Loss: 0.0180 Acc: 0.9957\n","\n","Epoch 19/50\n","----------\n","train Loss: 0.0116 Acc: 0.9963\n","val Loss: 0.0218 Acc: 0.9935\n","\n","Epoch 20/50\n","----------\n","train Loss: 0.0102 Acc: 0.9975\n","val Loss: 0.0159 Acc: 0.9957\n","\n","Epoch 21/50\n","----------\n","train Loss: 0.0135 Acc: 0.9963\n","val Loss: 0.0191 Acc: 0.9957\n","\n","Epoch 22/50\n","----------\n","train Loss: 0.0154 Acc: 0.9944\n","val Loss: 0.0175 Acc: 0.9957\n","\n","Epoch 23/50\n","----------\n","train Loss: 0.0139 Acc: 0.9963\n","val Loss: 0.0156 Acc: 0.9957\n","\n","Epoch 24/50\n","----------\n","train Loss: 0.0161 Acc: 0.9932\n","val Loss: 0.0161 Acc: 0.9957\n","\n","Epoch 25/50\n","----------\n","train Loss: 0.0142 Acc: 0.9957\n","val Loss: 0.0191 Acc: 0.9957\n","\n","Epoch 26/50\n","----------\n","train Loss: 0.0229 Acc: 0.9932\n","val Loss: 0.0162 Acc: 0.9957\n","\n","Epoch 27/50\n","----------\n","train Loss: 0.0249 Acc: 0.9932\n","val Loss: 0.0151 Acc: 0.9957\n","\n","Epoch 28/50\n","----------\n","train Loss: 0.0147 Acc: 0.9944\n","val Loss: 0.0221 Acc: 0.9957\n","\n","Epoch 29/50\n","----------\n","train Loss: 0.0123 Acc: 0.9963\n","val Loss: 0.0206 Acc: 0.9957\n","\n","Epoch 30/50\n","----------\n","train Loss: 0.0081 Acc: 0.9969\n","val Loss: 0.0205 Acc: 0.9957\n","\n","Epoch 31/50\n","----------\n","train Loss: 0.0160 Acc: 0.9957\n","val Loss: 0.0178 Acc: 0.9957\n","\n","Epoch 32/50\n","----------\n","train Loss: 0.0070 Acc: 0.9981\n","val Loss: 0.0172 Acc: 0.9957\n","\n","Epoch 33/50\n","----------\n","train Loss: 0.0154 Acc: 0.9963\n","val Loss: 0.0159 Acc: 0.9957\n","\n","Epoch 34/50\n","----------\n","train Loss: 0.0181 Acc: 0.9944\n","val Loss: 0.0213 Acc: 0.9957\n","\n","Epoch 35/50\n","----------\n","train Loss: 0.0146 Acc: 0.9950\n","val Loss: 0.0184 Acc: 0.9957\n","\n","Epoch 36/50\n","----------\n","train Loss: 0.0114 Acc: 0.9975\n","val Loss: 0.0224 Acc: 0.9957\n","\n","Epoch 37/50\n","----------\n","train Loss: 0.0105 Acc: 0.9963\n","val Loss: 0.0169 Acc: 0.9957\n","\n","Epoch 38/50\n","----------\n","train Loss: 0.0181 Acc: 0.9957\n","val Loss: 0.0247 Acc: 0.9935\n","\n","Epoch 39/50\n","----------\n","train Loss: 0.0101 Acc: 0.9975\n","val Loss: 0.0228 Acc: 0.9935\n","\n","Epoch 40/50\n","----------\n","train Loss: 0.0123 Acc: 0.9963\n","val Loss: 0.0180 Acc: 0.9957\n","\n","Epoch 41/50\n","----------\n","train Loss: 0.0073 Acc: 0.9988\n","val Loss: 0.0172 Acc: 0.9957\n","\n","Epoch 42/50\n","----------\n","train Loss: 0.0139 Acc: 0.9950\n","val Loss: 0.0157 Acc: 0.9957\n","\n","Epoch 43/50\n","----------\n","train Loss: 0.0168 Acc: 0.9957\n","val Loss: 0.0177 Acc: 0.9957\n","\n","Epoch 44/50\n","----------\n","train Loss: 0.0118 Acc: 0.9969\n","val Loss: 0.0170 Acc: 0.9957\n","\n","Epoch 45/50\n","----------\n","train Loss: 0.0106 Acc: 0.9969\n","val Loss: 0.0192 Acc: 0.9957\n","\n","Epoch 46/50\n","----------\n","train Loss: 0.0102 Acc: 0.9963\n","val Loss: 0.0169 Acc: 0.9957\n","\n","Epoch 47/50\n","----------\n","train Loss: 0.0155 Acc: 0.9950\n","val Loss: 0.0188 Acc: 0.9957\n","\n","Epoch 48/50\n","----------\n","train Loss: 0.0152 Acc: 0.9957\n","val Loss: 0.0150 Acc: 0.9957\n","\n","Epoch 49/50\n","----------\n","train Loss: 0.0128 Acc: 0.9957\n","val Loss: 0.0160 Acc: 0.9957\n","\n","Epoch 50/50\n","----------\n","train Loss: 0.0129 Acc: 0.9969\n","val Loss: 0.0172 Acc: 0.9978\n","\n","Training complete in 11m 31s\n","Best val Acc: 0.997826\n"]}]},{"cell_type":"markdown","source":["# Load model"],"metadata":{"id":"bsdHiGIXsuKN"}},{"cell_type":"code","source":["load_model = models.resnet18(pretrained=True).cuda()\n","num_ftrs = load_model.fc.in_features\n","load_model.fc = nn.Linear(num_ftrs, 7)\n","load_model = load_model.to(device)\n","load_model.load_state_dict(torch.load('/content/drive/MyDrive/COURSES/CS232/CBMIR/model/CBMIR_resnet18_method3.pt'))\n","load_model = torch.nn.Sequential(*list(load_model.children())[:-1])"],"metadata":{"id":"0DUs9biRNX38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"],"metadata":{"id":"_6YTFTUDEZCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcI3l3vjqZ6P","executionInfo":{"status":"ok","timestamp":1653204428199,"user_tz":-420,"elapsed":7,"user":{"displayName":"An Võ Khánh","userId":"03935501821652710257"}},"outputId":"0864b03d-5ab1-49a4-95f7-328883a5a8f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-20          [-1, 128, 28, 28]             256\n","             ReLU-21          [-1, 128, 28, 28]               0\n","           Conv2d-22          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-23          [-1, 128, 28, 28]             256\n","           Conv2d-24          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-25          [-1, 128, 28, 28]             256\n","             ReLU-26          [-1, 128, 28, 28]               0\n","       BasicBlock-27          [-1, 128, 28, 28]               0\n","           Conv2d-28          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-29          [-1, 128, 28, 28]             256\n","             ReLU-30          [-1, 128, 28, 28]               0\n","           Conv2d-31          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-36          [-1, 256, 14, 14]             512\n","             ReLU-37          [-1, 256, 14, 14]               0\n","           Conv2d-38          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-39          [-1, 256, 14, 14]             512\n","           Conv2d-40          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-41          [-1, 256, 14, 14]             512\n","             ReLU-42          [-1, 256, 14, 14]               0\n","       BasicBlock-43          [-1, 256, 14, 14]               0\n","           Conv2d-44          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-45          [-1, 256, 14, 14]             512\n","             ReLU-46          [-1, 256, 14, 14]               0\n","           Conv2d-47          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-48          [-1, 256, 14, 14]             512\n","             ReLU-49          [-1, 256, 14, 14]               0\n","       BasicBlock-50          [-1, 256, 14, 14]               0\n","           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n","             ReLU-53            [-1, 512, 7, 7]               0\n","           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n","           Conv2d-56            [-1, 512, 7, 7]         131,072\n","      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n","             ReLU-58            [-1, 512, 7, 7]               0\n","       BasicBlock-59            [-1, 512, 7, 7]               0\n","           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n","             ReLU-62            [-1, 512, 7, 7]               0\n","           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n","             ReLU-65            [-1, 512, 7, 7]               0\n","       BasicBlock-66            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","================================================================\n","Total params: 11,176,512\n","Trainable params: 11,176,512\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 62.79\n","Params size (MB): 42.64\n","Estimated Total Size (MB): 105.99\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOeyZccMmHEC"},"outputs":[],"source":["faiss_index = faiss.IndexFlatL2(512)   # build the index\n","collection = []\n","# storing the image representations\n","im_indices = []\n","with torch.no_grad():\n","    for f in glob.glob(os.path.join(PATH_TRAIN, '*/*')):\n","        im = pil_loader(f)\n","        im = im.resize((224,224))\n","        im = torch.tensor([val_transforms(im).numpy()]).cuda()\n","\n","        preds = load_model(im)\n","        preds = np.array([preds[0].cpu().numpy().flatten()])\n","        faiss_index.add(preds) #add the representation to index\n","        im_indices.append(f)   #store the image name to find it later on\n","    \n","    for f in glob.glob(os.path.join(PATH_VALID, '*/*')):\n","        im = pil_loader(f)\n","        im = im.resize((224,224))\n","        im = torch.tensor([val_transforms(im).numpy()]).cuda()\n","\n","        preds = load_model(im)\n","        preds = np.array([preds[0].cpu().numpy().flatten()])\n","        faiss_index.add(preds) #add the representation to index\n","        im_indices.append(f)   #store the image name to find it later on"]},{"cell_type":"code","source":["pickle_out = open(\"/content/drive/MyDrive/COURSES/CS232/CBMIR/collection/collection_resnet18_method3.pickle\", \"wb\")\n","pickle.dump(faiss_index, pickle_out)\n","pickle_out.close()\n","\n","np.save(\"/content/drive/MyDrive/COURSES/CS232/CBMIR/collection/im_indices_resnet18_method3.npy\", im_indices)"],"metadata":{"id":"udhzymeHbYmf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing\n"],"metadata":{"id":"LCdgBhBns7wn"}},{"cell_type":"code","source":["pickle_in = open(\"/content/drive/MyDrive/COURSES/CS232/CBMIR/collection/collection_resnet18_method3.pickle\", \"rb\")\n","load_faiss_index = pickle.load(pickle_in)\n","load_im_indices = np.load(\"/content/drive/MyDrive/COURSES/CS232/CBMIR/collection/im_indices_resnet18_method3.npy\")"],"metadata":{"id":"8Ty5I1xtdkhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQ4i_uUFvvQP"},"outputs":[],"source":["PATH_TEST = \"/content/drive/MyDrive/COURSES/CS232/CBMIR/ct_dataset_split/test\"\n","# Retrieval with a query image\n","list_ap = []\n","with torch.no_grad():\n","    for folder in os.listdir(PATH_TEST):\n","        for f in os.listdir(os.path.join(PATH_TEST, folder)):\n","            im = pil_loader(os.path.join(PATH_TEST, folder, f))\n","            print('query:', os.path.join(PATH_TEST, folder, f))\n","            im = im.resize((224,224))\n","            im = torch.tensor([val_transforms(im).numpy()]).cuda()\n","\n","            test_embed = load_model(im)\n","\n","            test_embed = np.array([test_embed[0].cpu().numpy().flatten()])\n","            # break\n","            _, I = load_faiss_index.search(test_embed, 15)\n","            # print(\"Retrieved Image: {}\".format(load_im_indices[I[0][0]]))\n","\n","            correct = 0\n","            ap = 0\n","            for i in range(15):\n","                pred = load_im_indices[I[0][i]].split('/')[-2]\n","                print(\"Retrieved Image:\", load_im_indices[I[0][i]])\n","                if pred == folder:\n","                  correct += 1\n","                  ap += correct / (i + 1)\n","            ap /= correct    \n","            list_ap.append(ap)\n","            print('correct:', correct)\n","            print('ap:', ap)\n","            print('\\n\\n/--------------------------------------/')"]},{"cell_type":"code","source":["import statistics\n","map = statistics.mean(list_ap)\n","map"],"metadata":{"id":"RgoZM5UYNYxZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltylBtlPmMUP"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"name":"CBMIR_resnet18_method3.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1NcOCTX_vHSg8sju6h-2-cwmEgxeXDm6L","authorship_tag":"ABX9TyNxi/AtsEkyb/G20e8asXEA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}